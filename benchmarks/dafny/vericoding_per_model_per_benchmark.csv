model,benchmark,total_files,successes,failures,success_rate_pct
claude-opus,apps,677,451,226,66.61742983751846
claude-opus,bignum,62,9,53,14.516129032258064
claude-opus,dafnybench,443,297,146,67.04288939051919
claude-opus,humaneval,166,141,25,84.93975903614458
claude-opus,verified-cogen,172,156,16,90.69767441860465
claude-opus,verina,157,115,42,73.2484076433121
claude-sonnet,apps,677,408,269,60.2658788774003
claude-sonnet,bignum,0,0,0,0.0
claude-sonnet,dafnybench,443,271,172,61.17381489841986
claude-sonnet,humaneval,166,119,47,71.6867469879518
claude-sonnet,verified-cogen,172,145,27,84.30232558139535
claude-sonnet,verina,157,111,46,70.70063694267516
deepseek,apps,677,207,470,30.576070901033972
deepseek,bignum,62,2,60,3.225806451612903
deepseek,dafnybench,443,134,309,30.248306997742663
deepseek,humaneval,166,72,94,43.373493975903614
deepseek,verified-cogen,172,86,86,50.0
deepseek,verina,157,49,108,31.21019108280255
gemini,apps,677,276,401,40.76809453471196
gemini,bignum,62,1,61,1.6129032258064515
gemini,dafnybench,443,258,185,58.239277652370205
gemini,humaneval,166,97,69,58.433734939759034
gemini,verified-cogen,172,137,35,79.65116279069767
gemini,verina,157,76,81,48.40764331210191
gemini-flash,apps,677,196,481,28.95125553914328
gemini-flash,bignum,62,0,62,0.0
gemini-flash,dafnybench,443,161,282,36.3431151241535
gemini-flash,humaneval,166,73,93,43.975903614457835
gemini-flash,verified-cogen,172,98,74,56.97674418604651
gemini-flash,verina,157,64,93,40.76433121019108
glm,apps,677,330,347,48.74446085672083
glm,bignum,62,3,59,4.838709677419355
glm,dafnybench,443,90,353,20.31602708803612
glm,humaneval,0,0,0,0.0
glm,verified-cogen,172,116,56,67.44186046511628
glm,verina,157,49,108,31.21019108280255
gpt,apps,677,468,209,69.12850812407682
gpt,bignum,62,12,50,19.35483870967742
gpt,dafnybench,443,283,160,63.88261851015801
gpt,humaneval,0,0,0,0.0
gpt,verified-cogen,172,149,23,86.62790697674419
gpt,verina,157,83,74,52.86624203821656
gpt-mini,apps,677,485,192,71.63958641063516
gpt-mini,bignum,62,16,46,25.806451612903224
gpt-mini,dafnybench,443,284,159,64.10835214446952
gpt-mini,humaneval,166,137,29,82.53012048192771
gpt-mini,verified-cogen,172,147,25,85.46511627906976
gpt-mini,verina,157,91,66,57.961783439490446
grok-code,apps,677,358,319,52.88035450516987
grok-code,bignum,62,2,60,3.225806451612903
grok-code,dafnybench,443,234,209,52.8216704288939
grok-code,humaneval,0,0,0,0.0
grok-code,verified-cogen,172,130,42,75.5813953488372
grok-code,verina,157,71,86,45.22292993630573
