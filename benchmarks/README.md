# Original Sources

## DafnyBench
* `benchmarks/dafny/dafnybench`
* https://github.com/sun-wendy/DafnyBench

## HumanEval, Clever
* `benchmarks/dafny/humaneval`
* `benchmarks/lean/clever`
* https://github.com/openai/human-eval
* https://github.com/JetBrains-Research/HumanEval-Dafny
* https://github.com/trishullab/clever

The Dafny files were mostly translated from the original Python source. Some of the Dafny files were taken from Jetbrains Research's HumanEval Dafny repo.

The Lean files were taken from the Clever benchmark which were originally derived from the HumanEval benchmark. HumanEval problems 22, 137, 162 are missing from Clever for reasons mentioned in the Clever paper.

## NumPyTriple
* `benchmarks/lean/numpy_triple`
* Derived from NumPy documentation
* Specs in new Hoare triple format

## NumPySimple
* `benchmarks/lean/numpy_simple`
* Derived from NumPy documentation
* Specs in classical Lean format

## Verina
* `benchmarks/lean/verina`
* https://github.com/sunblaze-ucb/verina

## APPS
* `benchmarks/dafny/apps`
* https://github.com/hendrycks/apps

## FVAPPS
* `benchmarks/lean/fvapps`
* https://huggingface.co/datasets/quinn-dougherty/fvapps

## VerifiedCogen
* `benchmarks/verus/verified_cogen`
* https://github.com/JetBrains-Research/verified-cogen


## BigNum
* `benchmarks/dafny/bignum`
* Written from scratch
