# Vericoding - Raw Benchmarks

## DafnyBench
* `benchmarks/dafny/dafnybench`
* https://github.com/sun-wendy/DafnyBench

## HumanEval, Clever
* `benchmarks/lean/humaneval`
* https://github.com/openai/human-eval
* https://github.com/trishullab/clever

Contains the Lean files from the Clever benchmark which were originally derived from the HumanEval benchmark. HumanEval problems 22, 137, 162 are missing from Clever for reasons mentioned in the Clever paper.

## NumPy
* `benchmarks/lean/numpy_triple`
* (BAIF internal) Derived from NumPy documentation.

## Verina
* `benchmarks/lean/verina`
* https://github.com/sunblaze-ucb/verina

## APPS
* https://github.com/hendrycks/apps